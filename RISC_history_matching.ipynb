{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History matching with RISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "d = '../../RISC/'\n",
    "\n",
    "k = 25  # number of ensembles run\n",
    "sigma = 3\n",
    "OUTPUTS = ('n small', 'n medium', 'n large')\n",
    "observations = pd.read_csv(d + 'farm-size-year.csv')\n",
    "YEARS = range(13)\n",
    "TOTAL_YEARS = 13\n",
    "# drivers are in the order given in fig 10 of the paper\n",
    "drivers = (\n",
    "          (False, False, False, False), (False, False, False, True),\n",
    "          (False, False, True, True), (False, False, True, False),\n",
    "          (False, True, False, True), (False, True, False, False),\n",
    "          (False, True, True, True), (False, True, True, False),\n",
    "          (True, False, False, True), (True, False, False, False),\n",
    "          (True, False, True, True), (True, False, True, False),\n",
    "          (True, True, False, True), (True, True, False, False),\n",
    "          (True, True, True, True), (True, True, True, False))\n",
    "\n",
    "\n",
    "# set automatically\n",
    "v_ens = 0\n",
    "m_ens = 0\n",
    "\n",
    "\n",
    "def get_fp(driver):\n",
    "    # Get the filepath of a driver\n",
    "    fp = d + 'results_non_variance/farm-ensembles-'\n",
    "    fp = fp + 'true-' if driver[0] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[1] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[2] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[3] else fp + 'false-'\n",
    "    return fp + '1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are analysing is a time series. We must somehow measure the difference between the estimated time series (from the model outputs) and the expected time series (from the empirical data). I looked at a few different ways of doing this.\n",
    "\n",
    "Initially, I measured correlation given we're mainly interested in the direction of the trend rather than magnitude of, say, farms of a small size. However, looking at only correlation produced results that poorly reflected the actual variance in the model when variance-rate was included (I'm skipping over the why of this in here for now, but I can discuss it and add it if desired).\n",
    "\n",
    "I then looked at a few different methods of measuring the size of the error between estimated and expected time series data. These were \n",
    "- mean absolute error\n",
    "- root mean square error\n",
    "- percentage error, and\n",
    "- mean absolute scaled error.\n",
    "\n",
    "I found the final one (mean absolute scaled error) to be the most appropriate. This was a subjective decision, made by visually comparing plots of the time series against the calculated errors, and choosing the method which produced a result that I felt appropriately matched the error I saw in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_scaled_error(observations_col, estimates_col):\n",
    "    \"\"\" The observation and estimate must be a single column (output).\"\"\"\n",
    "    result = 0\n",
    "    # mean absolute error\n",
    "    mae = np.mean([abs(observations_col[year] - \n",
    "                       estimates_col[year])\n",
    "                    for year in YEARS])\n",
    "    denom = sum([abs(observations_col[year] - observations_col[year-1])\n",
    "                 for year in YEARS[1:]]) / (TOTAL_YEARS - 1)\n",
    "    for year in YEARS:\n",
    "        obs = observations_col[year]\n",
    "        est = estimates_col[year]\n",
    "        error = obs - est\n",
    "        result += error / denom\n",
    "    return abs(result / TOTAL_YEARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above to calculate the mean absolute squared error (MASE) between one output of a simulation and the related empirical data, or between two different simulation runs.\n",
    "\n",
    "The model has 16 different drivers we are performing history matching on.\n",
    "For each driver we have results from an ensemble of 25 runs.\n",
    "For a given driver and a given model output, we quantify the magnitude of difference between the 25 ensemble runs using MASE, comparing each run with each other run. We then calculate the variance of these differences to obtain the variance of the ensembles for the given driver and output.\n",
    "It is common in history matching to take the mean variance across all models (i.e. across all drivers).\n",
    "We obtain a single measured variance for each model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_ens_X(driver_indexes):\n",
    "    \"\"\" Calculate the average variance across plausible ensemble runs.\"\"\"\n",
    "    results = dict((c, 0) for c in OUTPUTS)\n",
    "    for di in driver_indexes:\n",
    "        results_x = v_ens_x(drivers[di])\n",
    "        for output in OUTPUTS:\n",
    "            results[output] += results_x[output]\n",
    "    return dict((c, results[c]/len(driver_indexes)) for c in OUTPUTS)\n",
    "\n",
    "\n",
    "def v_ens_x(driver):\n",
    "    \"\"\" Calculate the variance of a plausible ensemble.\"\"\"\n",
    "    results = dict((c, []) for c in OUTPUTS)\n",
    "    for output in OUTPUTS:\n",
    "        r = []\n",
    "        for i in range(k):\n",
    "            df = pd.read_csv(get_fp(driver)[:-5] + str(i+1) + '.csv')\n",
    "            r.append(df[output].tolist())\n",
    "        for a in range(k-1):\n",
    "            for b in range(a+1, k):\n",
    "                results[output].append(max(mean_absolute_scaled_error(r[a], r[b]),\n",
    "                                        mean_absolute_scaled_error(r[b], r[a])))\n",
    "    return dict((c, np.var(results[c], ddof=1)) for c in OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n large': 0.035835304631760136,\n",
       " 'n medium': 0.044756856128314475,\n",
       " 'n small': 0.03637738203281682}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what the variance is for each column\n",
    "v_ens_X(range(len(drivers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's little variance in the ensembles.\n",
    "\n",
    "Now let's do some history matching. We quantify the implausibility of a driver by measuring the MASE between a single run of the model against the empiricial data for each output. Each MASE (one per output) is then divided by the square root of the ensemble variance for the given output. So, we have a separate measure of implausibility per output.\n",
    "\n",
    "Generally, if the model is implausible for one output, then we say it is an implausible model, even if its results for other outputs are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave  1\n",
      "Implausiblity scores:  [127.05, 60.78, 51.21, 60.12, 46.58, 50.67, 62.82, 41.28, 155.74, 157.32, 145.71, 147.08, 34.11, 34.43, 34.0, 35.77]\n",
      "New plausible drivers: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for output in OUTPUTS:\n",
    "        est = estimates[output].tolist()\n",
    "        obs = observations[output].tolist()\n",
    "        diff = mean_absolute_scaled_error(obs, est)\n",
    "        # Take the maximum implausiblity.\n",
    "        # If the model is implausible for one output\n",
    "        # then we're not interested in it.\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[output]))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "\n",
    "def wave(plaus_space):\n",
    "    \"\"\" Run a single wave of history matching.\n",
    "\n",
    "        plaus_space: index of plausible drivers to test.\"\"\"\n",
    "    globals()['v_ens'] = v_ens_X(plaus_space)\n",
    "    new_plaus_space = []\n",
    "    implaus_scores = []\n",
    "    for i in plaus_space:\n",
    "        score = implaus(drivers[i])\n",
    "        implaus_scores.append(score)\n",
    "        if score < sigma:\n",
    "            new_plaus_space.append(i)\n",
    "    return new_plaus_space, implaus_scores\n",
    "\n",
    "\n",
    "def run_history_matching():\n",
    "    \"\"\" Run waves of history matching until the new plausible space\n",
    "        is either empty (the whole space is implausible)\n",
    "        or is unchanged from the previous wave.\n",
    "    \"\"\"\n",
    "    wave_i = 1\n",
    "    plaus_space = []\n",
    "    new_plaus_space = range(len(drivers))\n",
    "    while len(new_plaus_space) > 0 and len(plaus_space) != len(new_plaus_space):\n",
    "        plaus_space = new_plaus_space\n",
    "        new_plaus_space, implaus_scores = wave(plaus_space)\n",
    "        print('Wave ', str(wave_i))\n",
    "        print('Implausiblity scores: ', implaus_scores)\n",
    "        print('New plausible drivers:', new_plaus_space)\n",
    "        print('')\n",
    "        wave_i += 1\n",
    "\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plausible space is empty. In general, an implausibility score of 3 is used as a threshold. A score of less than 3 is considered plausible, and more than 3 is considered implausible.\n",
    "\n",
    "None of the models have obtained a plausible score. This has occurred because there is more uncertainty in the model than we are quantifying. We're only quantifying uncertainty stemming from the variance of the ensembles, but there is very little variance. The models do not perfectly match the empirical data, and we must quantify this error. This is often referred to as *model discrepancy*, *model inadequancy* or *uncertainty in model form*.\n",
    "\n",
    "We can quantify the model discrepancy by measuring the MASE between each of the 16 possible models and the empirical data. We calculate the average error between the model output and data across all models. We do this separately for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n large': 14.725075204125483, 'n medium': 5.371679110956763, 'n small': 7.079308403460946}\n"
     ]
    }
   ],
   "source": [
    "def m_ens_X(plaus_space):\n",
    "    \"\"\" Calculate model discrepancy.\n",
    "        Calculate the average model discrepency across all plausible models.\"\"\"\n",
    "    results = dict((output, 0) for output in OUTPUTS)\n",
    "    total_models = len(plaus_space)\n",
    "    for di in plaus_space:\n",
    "        df = pd.read_csv(get_fp(drivers[di]))\n",
    "        for output in OUTPUTS:\n",
    "            est = df[output].tolist()\n",
    "            obs = observations[output].tolist()\n",
    "            e = mean_absolute_scaled_error(obs, est)\n",
    "            results[output] += e\n",
    "    return dict((c, results[c]/len(plaus_space)) for c in OUTPUTS)\n",
    "\n",
    "print(m_ens_X(range(len(drivers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the model discrepancy for each output. The uncertainty in the model's ability to match the empirical data is clearly much higher than the ensemble variance of the models. Therefore, it is more likely to have an effect on the implausibility of the models.\n",
    "\n",
    "First, we must update the implaus and wave functions to use this new model discrepancy term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave  1\n",
      "Implausiblity scores:  [6.2602, 3.3365, 3.2269, 3.3245, 3.0165, 3.0549, 3.1869, 2.9512, 7.6736, 7.7514, 7.1796, 7.2467, 1.6805, 1.6966, 1.6751, 1.7624]\n",
      "New plausible drivers: [7, 12, 13, 14, 15]\n",
      "\n",
      "Wave  2\n",
      "Implausiblity scores:  [4.8274, 2.4754, 2.4991, 2.4674, 2.596]\n",
      "New plausible drivers: [12, 13, 14, 15]\n",
      "\n",
      "Wave  3\n",
      "Implausiblity scores:  [2.522, 2.5461, 2.5139, 2.6448]\n",
      "New plausible drivers: [12, 13, 14, 15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for output in OUTPUTS:\n",
    "        est = estimates[output].tolist()\n",
    "        obs = observations[output].tolist()\n",
    "        diff = mean_absolute_scaled_error(obs, est)\n",
    "        # Take the maximum implausiblity.\n",
    "        # If the model is implausible for one output\n",
    "        # then we're not interested in it.\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[output] + m_ens[output]))\n",
    "    return round(implaus, 4)\n",
    "\n",
    "\n",
    "def wave(plaus_space):\n",
    "    \"\"\" Run a wave of history matching.\n",
    "\n",
    "        plaus_space: index of plausible drivers to test.\"\"\"\n",
    "    globals()['v_ens'] = v_ens_X(plaus_space)\n",
    "    globals()['m_ens'] = m_ens_X(plaus_space)\n",
    "    new_plaus_space = []\n",
    "    implaus_scores = []\n",
    "    for i in plaus_space:\n",
    "        score = implaus(drivers[i])\n",
    "        implaus_scores.append(score)\n",
    "        if score < sigma:\n",
    "            new_plaus_space.append(i)\n",
    "    return new_plaus_space, implaus_scores\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three waves of history matching occurred. In the first, all but drivers 7, 12, 13, 14 and 15 were removed from the plausible space.\n",
    "\n",
    "In the second wave, driver 7 was removed.\n",
    "\n",
    "The result from the third wave is the same as the second, so we end the procedure.\n",
    "\n",
    "We found the final four drivers listed to be plausible. This matches with the results found in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
