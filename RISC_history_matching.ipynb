{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "d = '../../RISC/'\n",
    "\n",
    "k = 25  # number of ensembles run\n",
    "sigma = 3\n",
    "COLS = ('n small', 'n medium', 'n large')\n",
    "observations = pd.read_csv(d + 'farm-size-year.csv')\n",
    "# drivers are in the order given in fig 10 of the paper\n",
    "drivers = (\n",
    "          (False, False, False, False), (False, False, False, True),\n",
    "          (False, False, True, True), (False, False, True, False),\n",
    "          (False, True, False, True), (False, True, False, False),\n",
    "          (False, True, True, True), (False, True, True, False),\n",
    "          (True, False, False, True), (True, False, False, False),\n",
    "          (True, False, True, True), (True, False, True, False),\n",
    "          (True, True, False, True), (True, True, False, False),\n",
    "          (True, True, True, True), (True, True, True, False))\n",
    "\n",
    "\n",
    "# set automatically\n",
    "v_ens = 0\n",
    "m_ens = 0\n",
    "\n",
    "\n",
    "def get_fp(driver):\n",
    "    # Get the filepath of a driver\n",
    "    fp = d + 'results/farm-ensembles-'\n",
    "    fp = fp + 'true-' if driver[0] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[1] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[2] else fp + 'false-'\n",
    "    fp = fp + 'true-' if driver[3] else fp + 'false-'\n",
    "    return fp + '1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create functions to calculate the variance of each model output across 25 ensemble runs.\n",
    "The variance of a given output is calculated by measuring the correlation of each ensemble run compared with each other run (for a given driver). The variance across these comparisons is then calculated. The average variance across all drivers is then used for history matching.\n",
    "From the literature, it seems that you generally have a separate evaluation of ensemble variance per output.\n",
    "\n",
    "\n",
    "The correlation c gives a result such that 0 <= |c| <= 1. I've found this was not ideal as you start dividing small numbers (i.e. <1) by other small numbers, ending up with tiny implausibility scores making everything \"plausible\". I've converted all measures of correlation in the model to a measure of error in the range [0, 100], **but, of course, this may need to be reconsidered.**. The error is e = (1 - c) * 50, such that 0 indicates no error (perfect positive correlation), 50 indicates no correlation, and 100 inidicates perfect negative correlation.\n",
    "\n",
    "**I notice that if I instead normalise error to the range [0, 1], then I need to use a implausibility threshold of 0.3 instead of 3. All papers I've seen use a threshold of 3, so I find this curious.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_ens_X(driver_indexes):\n",
    "    \"\"\" Calculate the average variance across plausible ensemble runs.\"\"\"\n",
    "    results = dict((c, 0) for c in COLS)\n",
    "    for di in driver_indexes:\n",
    "        results_x = v_ens_x_corr(drivers[di])\n",
    "        for col in COLS:\n",
    "            results[col] += results_x[col]\n",
    "    return dict((c, results[c]/len(driver_indexes)) for c in COLS)\n",
    "\n",
    "\n",
    "def v_ens_x_corr(driver):\n",
    "    \"\"\" Calculate the variance of a plausible ensemble.\"\"\"\n",
    "    results = dict((c, []) for c in COLS)\n",
    "    for col in COLS:\n",
    "        r = []\n",
    "        for i in range(k):\n",
    "            df = pd.read_csv(get_fp(driver)[:-5] + str(i+1) + '.csv')\n",
    "            r.append(df[col].tolist())\n",
    "        for a in range(k):\n",
    "            for b in range(a, k):\n",
    "                results[col].append(stats.spearmanr(r[a], r[b])[0] * 50)\n",
    "    return dict((c, np.var(results[c], ddof=1)) for c in COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n small': 0.002090608439150047, 'n medium': 1.3517347526384984, 'n large': 0.5342640874016333}\n"
     ]
    }
   ],
   "source": [
    "print(v_ens_X(range(len(drivers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances are quite small.\n",
    "\n",
    "Next, are the functions to calculate the implaubility of a given driver and to run waves of history matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for col in COLS:\n",
    "        this_obs = observations[col].tolist()\n",
    "        this_est = estimates[col].tolist()\n",
    "        diff = (1 - stats.spearmanr(this_obs, this_est).correlation) * 50\n",
    "        implaus = max(implaus, diff / np.sqrt(v_ens[col]))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "\n",
    "def wave(plaus_space):\n",
    "    \"\"\" Run a wave of history matching.\n",
    "\n",
    "        plaus_space: index of plausible drivers to test.\"\"\"\n",
    "    globals()['v_ens'] = v_ens_X(plaus_space)\n",
    "    new_plaus_space = []\n",
    "    implaus_scores = []\n",
    "    for i in plaus_space:\n",
    "        score = implaus(drivers[i])\n",
    "        implaus_scores.append(score)\n",
    "        if score < sigma:\n",
    "            new_plaus_space.append(i)\n",
    "    return new_plaus_space, implaus_scores\n",
    "\n",
    "\n",
    "def run_history_matching():\n",
    "    \"\"\" Run waves of history matching until the new plausible space\n",
    "        is either empty (the whole space is implausible)\n",
    "        or is unchanged from the previous wave.\n",
    "    \"\"\"\n",
    "    plaus_space = []\n",
    "    new_plaus_space = range(len(drivers))\n",
    "    while len(new_plaus_space) > 0 and len(plaus_space) != len(new_plaus_space):\n",
    "        plaus_space = new_plaus_space\n",
    "        new_plaus_space, implaus_scores = wave(plaus_space)\n",
    "        print('implausiblity scores: ', implaus_scores)\n",
    "        print('new plausible space:', new_plaus_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implausiblity scores:  [2145.02, 2145.02, 2145.02, 2145.02, 2163.04, 2163.04, 2145.02, 2163.04, 85.91, 82.73, 80.65, 69.91, 42.06, 42.06, 42.06, 42.06]\n",
      "new plausible space: []\n"
     ]
    }
   ],
   "source": [
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implausibility of less than 3 indicates that the run had a plausible set of a parameters.\n",
    "These implausibility scores are all large because there is no variance in the ensembles and ensemble variance is the only uncertainty we've accounted for.\n",
    "The model isn't perfect - there is some uncertainty in its ability to correctly model the empirical data.\n",
    "Therefore, we must quantify the uncertainty in the model. This uncertainty is referred to as *model discrepency* or *model inadequacy*.\n",
    "\n",
    "How do we do quantify model discrepency? My ideas are, we could calculate\n",
    "1. One score across all plausible models and outputs (averaging their model discrepencies).\n",
    "2. One score per output, averaging the uncertainty of predicting each output across all plausible models.\n",
    "3. One score per model, averaging the uncertainty of the model's ability to predict all outputs.\n",
    "4. One score per model and per output - quantifying each model's ability to predict each output.\n",
    "\n",
    "The first two average model discrepancy across all models, the third and fourth have a separate score per model.\n",
    "The first and third average over outputs, whilst second and fourth have separate scores per output.\n",
    "\n",
    "Currently, ensemble variance is averaged across models but there's a separate ensemble variance measurement per output. This would fit with idea 2.\n",
    "\n",
    "We'll start with the first in the list.\n",
    "We measure model discrepency by calculating the error in correlation between the model results and the empirical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One score across all plausible models (averaging their model discrepencies)\n",
    "\n",
    "For a given model, we calculate this error (compliment of correlation) for all three outputs and take the largest result to represent the uncertainty of that model.\n",
    "We then average the model uncertainty across all plausible models.\n",
    "\n",
    "We must also update the implaus and wave functions to include the new model discrepency term m_ens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implausiblity scores:  [11.853961294015207, 11.903215662968012, 11.903215662968012, 11.903215662968012, 11.953574414132982, 11.953574414132982, 11.853961294015207, 11.953574414132982, 7.559995629312552, 7.280194617899933, 7.097775162781711, 6.152510713532746, 2.2108599716477055, 2.1309909075174063, 2.1114952225889168, 2.0618128480595215]\n",
      "new plausible space: [12, 13, 14, 15]\n",
      "implausiblity scores:  [4.364117158681839, 4.206459976549616, 4.1679765564290445, 4.069906255302645]\n",
      "new plausible space: []\n"
     ]
    }
   ],
   "source": [
    "def m_ens_X(plaus_space):\n",
    "    \"\"\" Calculate model discrepancy.\n",
    "        Calculate the average model discrepency across all plausible models.\"\"\"\n",
    "    E = 0\n",
    "    for di in plaus_space:\n",
    "        e = 0\n",
    "        df = pd.read_csv(get_fp(drivers[di]))\n",
    "        for col in COLS:\n",
    "            est = df[col].tolist()\n",
    "            obs = observations[col].tolist()\n",
    "            c = (1 - stats.spearmanr(est, obs).correlation) * 50\n",
    "            # take the maximum uncertainty across the model outputs\n",
    "            # This is a pessmistic approach;\n",
    "            # the uncertainty in the whole model is based on\n",
    "            # the greatest uncertainty of all outputs.\n",
    "            e = max(e, c)\n",
    "        E += e\n",
    "    return E / len(plaus_space)\n",
    "\n",
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for col in COLS:\n",
    "        this_obs = observations[col].tolist()\n",
    "        this_est = estimates[col].tolist()\n",
    "        diff = (1 - stats.spearmanr(this_obs, this_est).correlation) * 50\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[col] + m_ens))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "def wave(plaus_space):\n",
    "    \"\"\" Run a wave of history matching.\n",
    "\n",
    "        plaus_space: index of plausible drivers to test.\"\"\"\n",
    "    globals()['v_ens'] = v_ens_X(plaus_space)\n",
    "    globals()['m_ens'] = m_ens_X(plaus_space)\n",
    "    new_plaus_space = []\n",
    "    implaus_scores = []\n",
    "    for i in plaus_space:\n",
    "        score = implaus(drivers[i])\n",
    "        implaus_scores.append(score)\n",
    "        if score < sigma:\n",
    "            new_plaus_space.append(i)\n",
    "    return new_plaus_space, implaus_scores\n",
    "\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first wave of history matching, we find that only the final four drivers are plausible. This matches the results in the paper.\n",
    "In the second wave, none of the space is considered plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One score per output, averaging the uncertainty of predicting each output across all plausible models\n",
    "\n",
    "This method matches with how we measure ensemble variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implausiblity scores:  [14.008170770452926, 14.324739601423616, 14.324739601423616, 14.324739601423616, 13.964872359354736, 13.964872359354736, 13.84849842302678, 13.964872359354736, 12.201440336914766, 11.749856035235757, 11.455440508156956, 9.929832776930446, 3.568213178951998, 3.439308657238712, 3.407843634222041, 3.327658861857061]\n",
      "new plausible space: []\n"
     ]
    }
   ],
   "source": [
    "def m_ens_X(plaus_space):\n",
    "    \"\"\" Calculate model discrepancy.\n",
    "        Calculate the average model discrepency across all plausible models.\"\"\"\n",
    "    results = dict((col, 0) for col in COLS)\n",
    "    total_models = len(plaus_space)\n",
    "    for di in plaus_space:\n",
    "        df = pd.read_csv(get_fp(drivers[di]))\n",
    "        for col in COLS:\n",
    "            est = df[col].tolist()\n",
    "            obs = observations[col].tolist()\n",
    "            c = (1 - stats.spearmanr(est, obs).correlation) * 50\n",
    "            # calculate the average (across all models) implausibility of the output \n",
    "            results[col] += (c / total_models)\n",
    "    return results\n",
    "\n",
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for col in COLS:\n",
    "        this_obs = observations[col].tolist()\n",
    "        this_est = estimates[col].tolist()\n",
    "        diff = (1 - stats.spearmanr(this_obs, this_est).correlation) * 50\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[col] + m_ens[col]))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a model discrepancy term per output (averaged across models) has resulted in an empty plausibility space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One score per model, averaging the uncertainty of the model's ability to predict all outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implausiblity scores:  [11.729980875343529, 11.73649275418363, 11.73649275418363, 11.73649275418363, 12.030172878357602, 11.98721104857536, 12.2731288808095, 12.013883469192224, 13.32763548309953, 13.064357199025919, 12.889850810825871, 11.945429394165693, 6.755937697519662, 6.611571253941545, 6.5758941235716115, 6.484170343445727]\n",
      "new plausible space: []\n"
     ]
    }
   ],
   "source": [
    "def m_ens_X(plaus_space):\n",
    "    \"\"\" Calculate model discrepancy.\n",
    "        Calculate the average model discrepency across all plausible models.\"\"\"\n",
    "    results = dict((di, 0) for di in plaus_space)\n",
    "    for di in plaus_space:\n",
    "        df = pd.read_csv(get_fp(drivers[di]))\n",
    "        for col in COLS:\n",
    "            est = df[col].tolist()\n",
    "            obs = observations[col].tolist()\n",
    "            c = (1 - stats.spearmanr(est, obs).correlation) * 50\n",
    "            # calculate the averaged implausibility of the output\n",
    "            results[di] += (c / 3)\n",
    "    return results\n",
    "\n",
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for col in COLS:\n",
    "        this_obs = observations[col].tolist()\n",
    "        this_est = estimates[col].tolist()\n",
    "        diff = (1 - stats.spearmanr(this_obs, this_est).correlation) * 50\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[col] + m_ens[drivers.index(driver)]))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a model discrepancy term per model (averaging uncertainty over outputs) has resulted in an empty plausibility space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. One score per model and per output - quantifying each model's ability to predict each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implausiblity scores:  [9.903273828035172, 9.905399595207578, 9.905399595207578, 9.905399595207578, 9.94479805410099, 9.94479805410099, 9.903273828035172, 9.94479805410099, 7.890662362852677, 7.742011016977813, 7.643539903044827, 7.1114831193218215, 4.224209607349607, 4.145010809292585, 4.125448621692247, 4.075173649066611]\n",
      "new plausible space: []\n"
     ]
    }
   ],
   "source": [
    "def m_ens_X(plaus_space):\n",
    "    \"\"\" Calculate model discrepancy.\n",
    "        Calculate the average model discrepency across all plausible models.\"\"\"\n",
    "    results = dict((di, dict((col, 0) for col in COLS)) for di in plaus_space)\n",
    "    for di in plaus_space:\n",
    "        df = pd.read_csv(get_fp(drivers[di]))\n",
    "        for col in COLS:\n",
    "            est = df[col].tolist()\n",
    "            obs = observations[col].tolist()\n",
    "            c = (1 - stats.spearmanr(est, obs).correlation) * 50\n",
    "            # calculate the averaged implausibility of the output\n",
    "            results[di][col] = c\n",
    "    return results\n",
    "\n",
    "def implaus(driver):\n",
    "    \"\"\" Calculate the implausiblity of a set of parameters (driver).\"\"\"\n",
    "    estimates = pd.read_csv(get_fp(driver))\n",
    "    implaus = 0\n",
    "    for col in COLS:\n",
    "        this_obs = observations[col].tolist()\n",
    "        this_est = estimates[col].tolist()\n",
    "        diff = (1 - stats.spearmanr(this_obs, this_est).correlation) * 50\n",
    "        implaus = max(implaus,\n",
    "                     diff / np.sqrt(v_ens[col] + m_ens[drivers.index(driver)][col]))\n",
    "    return round(implaus, 2)\n",
    "\n",
    "\n",
    "run_history_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a model discrepancy term per model and per output has resulted in an empty plausibility space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having tested four different methods of quantifying the uncertainty in the model, only having a single value of uncertainty (averaged across all models and all outputs) has produced promising results.\n",
    "**Of course, this does not mean this is definitely the best approach. There may be something else that has been missed or should be considered.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
